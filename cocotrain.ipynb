{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6303df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "import json, pickle as pkl\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import COCODataset, COCODataLoader, ProcessedCOCOExample\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/lukovdg1/fiftyone/coco-2017/train' if necessary\n",
      "Found annotations at '/home/lukovdg1/fiftyone/coco-2017/raw/instances_train2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'coco-2017-train-None'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Filtering ids: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                   | 31288/90816 [02:07<04:10, 237.67it/s]"
     ]
    }
   ],
   "source": [
    "# 1. check dataloader\n",
    "ds = COCODataset(split=\"train\", max_samples=None, filterids={460872, 494550})\n",
    "#ds2 = COCODataset(split=\"valid\", max_samples=None, filterids={460872, 494550})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db23bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"qdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db7c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b41af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(x, size=512):\n",
    "    if x.size == (size, size):\n",
    "        return x\n",
    "    w, h = x.size\n",
    "    print(w, h)\n",
    "    s = min(w, h)\n",
    "    # compute maximum centercrop\n",
    "    w_remain, h_remain = w - s, h - s\n",
    "    x = x.crop((round(w_remain / 2), round(h_remain / 2), w - (w_remain - round(w_remain / 2)), h - (h_remain - round(h_remain / 2))))\n",
    "    print(x.size)\n",
    "    x = x.resize((size, size))\n",
    "    print(x.size)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e158ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catdogexample = deepcopy(ds.examples[0][1][0])\n",
    "catdogexample.image_data = crop_and_resize(catdogexample.load_image())\n",
    "catdogexample.image_path = None\n",
    "catdogexample.regions =[[np.array(crop_and_resize(Image.fromarray(region[0]))), region[1]] for region in catdogexample.regions]\n",
    "\n",
    "catdogexample.captions = [\"a white dog sitting next to a grey cat\"]\n",
    "print(catdogexample.captions)\n",
    "# print(catdogexample.regions)\n",
    "# print(catdogexample.regions[0])\n",
    "print([x[1] for x in catdogexample.regions])\n",
    "del catdogexample.regions[-1]\n",
    "\n",
    "print([x[1] for x in catdogexample.regions])\n",
    "print(catdogexample.regions[0][0].shape)\n",
    "catdogcopy = deepcopy(catdogexample)\n",
    "catdogcopy.regions = [[x for x in regionse] for regionse in catdogcopy.regions]\n",
    "catdogcopy.regions[0][1] = catdogexample.regions[1][1]\n",
    "catdogcopy.regions[1][1] = catdogexample.regions[0][1]\n",
    "print(catdogexample.regions)\n",
    "print(catdogcopy.regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52e27f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(catdogcopy.load_image())\n",
    "display(Image.fromarray(catdogcopy.regions[0][0]))\n",
    "print(catdogcopy.load_image().size)\n",
    "# ds.examples[0][1][0].regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84973b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a1abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "383a1bf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "applesorangesexample = deepcopy(ds.examples[0][1][1])\n",
    "display(applesorangesexample.load_image())\n",
    "print(applesorangesexample.load_image().size)\n",
    "\n",
    "applesorangesexample.image_data = crop_and_resize(applesorangesexample.load_image())\n",
    "applesorangesexample.image_path = None\n",
    "applesorangesexample.regions =[[np.array(crop_and_resize(Image.fromarray(region[0]))), region[1]] for region in applesorangesexample.regions]\n",
    "\n",
    "applesorangesexample.captions = [\"a red orange sitting beside a green apple\"]\n",
    "print(applesorangesexample.captions)\n",
    "# print(catdogexample.regions)\n",
    "# print(catdogexample.regions[0])\n",
    "print(applesorangesexample.regions[0][0].shape)\n",
    "\n",
    "print([x[1] for x in applesorangesexample.regions])\n",
    "print([x[0].astype(\"float\").sum() for x in applesorangesexample.regions])\n",
    "for region in applesorangesexample.regions:\n",
    "    pass #display(Image.fromarray(region[0]))\n",
    "    \n",
    "del applesorangesexample.regions[-1]\n",
    "del applesorangesexample.regions[0]\n",
    "applesorangesexample.regions[1][1] = \"apple\"\n",
    "\n",
    "applesorangescopy = deepcopy(applesorangesexample)\n",
    "applesorangescopy.regions = [[x for x in regionse] for regionse in applesorangescopy.regions]\n",
    "applesorangescopy.regions[0][1] = applesorangesexample.regions[1][1]\n",
    "applesorangescopy.regions[1][1] = applesorangesexample.regions[0][1]\n",
    "print(applesorangesexample.regions)\n",
    "print(applesorangescopy.regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0be9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_example(x):\n",
    "    img = x.load_image()\n",
    "    print(img.size)\n",
    "    display(img)\n",
    "    for caption in x.captions:\n",
    "        print(caption)\n",
    "    for region in x.regions:\n",
    "        print(f\"Region {region[1]} of size {region[0].shape}\")\n",
    "        display(Image.fromarray(region[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a313cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fba7bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_example(applesorangesexample)\n",
    "display_example(applesorangescopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1fee4c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_example(catdogexample)\n",
    "display_example(catdogcopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c98bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coco_dev_examples.dump\", \"wb\") as f:\n",
    "    pkl.dump([applesorangesexample, applesorangescopy, catdogexample, catdogcopy], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e63dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7ff2531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.array(Image.fromarray(catdogcopy.regions[0][0])) == catdogcopy.regions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16822de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b858ed2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'dataset.COCODataset'>: it's not the same object as dataset.COCODataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'dataset.COCODataset'>: it's not the same object as dataset.COCODataset"
     ]
    }
   ],
   "source": [
    "pkl.dumps(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a381ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCLIPTextEmbedder(FrozenCLIPEmbedder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n",
    "\n",
    "    def forward(self, fullprompt):\n",
    "        fullprompt, batsize = fullprompt if isinstance(fullprompt, tuple) else (fullprompt, 1)\n",
    "        \n",
    "        layer_idses = []\n",
    "        text_embeddingses = []\n",
    "        token_idses = []\n",
    "        \n",
    "        layertexts = fullprompt.split(\"\\n|\")\n",
    "        \n",
    "        # encode layers separately\n",
    "        for i, pos_prompt in enumerate(layertexts):\n",
    "            if i == 0:\n",
    "                token_ids, layer_ids = _tokenize_annotated_prompt(pos_prompt.strip(), self.tokenizer)\n",
    "                global_len = token_ids.shape[0]\n",
    "                global_bos_pos = 0\n",
    "                global_eos_pos = torch.nonzero(token_ids == self.tokenizer.eos_token_id)[0][0]\n",
    "            else:\n",
    "                pos_prompt = pos_prompt.strip()\n",
    "                token_ids = self.tokenizer([pos_prompt], return_tensors=\"pt\",\n",
    "                                        max_length=self.max_length, return_overflowing_tokens=False,\n",
    "                                        truncation=True)[\"input_ids\"][0]\n",
    "                layer_ids = torch.tensor([i] * token_ids.shape[0])\n",
    "            outputs = self.transformer(input_ids=token_ids[None].to(self.device), output_hidden_states=self.layer==\"hidden\")\n",
    "            if self.layer == \"last\":\n",
    "                z = outputs.last_hidden_state\n",
    "            elif self.layer == \"pooled\":\n",
    "                z = outputs.pooler_output[:, None, :]\n",
    "            else:\n",
    "                z = outputs.hidden_states[self.layer_idx]\n",
    "                \n",
    "            layer_idses.append(layer_ids)\n",
    "            token_idses.append(token_ids)\n",
    "            text_embeddingses.append(z)\n",
    "            \n",
    "        layer_ids = torch.cat(layer_idses, 0)[None].repeat(batsize, 1)\n",
    "        token_ids = torch.cat(token_idses, 0)[None].repeat(batsize, 1)\n",
    "        text_embeddings = torch.cat(text_embeddingses, 1).repeat(batsize, 1, 1)\n",
    "        global_prompt_mask = torch.zeros_like(token_ids)\n",
    "        global_bos_eos_mask = torch.zeros_like(global_prompt_mask)\n",
    "        global_prompt_mask[:, :global_len] = 1\n",
    "        global_bos_eos_mask[:, global_bos_pos] = 1\n",
    "        global_bos_eos_mask[:, global_eos_pos] = 1\n",
    "        \n",
    "        ret = CustomTextConditioning(text_embeddings, layer_ids.to(self.device), token_ids.to(self.device),\n",
    "                                     global_prompt_mask.to(self.device), global_bos_eos_mask.to(self.device))\n",
    "        return ret\n",
    "        # return text_embeddings, layer_ids.to(self.device), token_ids.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc49031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create model\n",
    "model_name = 'control_v11p_sd15_seg'\n",
    "model = create_model(f'./models/{model_name}.yaml').cpu()\n",
    "# load main weights\n",
    "model.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\n",
    "# load controlnet weights\n",
    "model.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\n",
    "model = model.cuda()\n",
    "\n",
    "# cast text encoder to our own\n",
    "model.cond_stage_model.__class__ = MyCLIPTextEmbedder\n",
    "\n",
    "for _module in model.model.diffusion_model.modules():\n",
    "    if _module.__class__.__name__ == \"CrossAttention\":\n",
    "        _module.__class__.forward = custom_forward\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ControlNetV11",
   "language": "python",
   "name": "control-v11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
